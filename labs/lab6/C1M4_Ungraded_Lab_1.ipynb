{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eeaef87-415f-412c-8c63-36ee09be365a",
   "metadata": {},
   "source": [
    "# Ungraded Lab - Exploring LLM Capabilities\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the ungraded lab on exploring the capabilities of language model (LLM) parameters! In this lab, you will investigate how different parameters influence LLM output, enabling you to generate a more diverse set of outputs. You will also learn to develop a method for allowing an LLM to maintain conversation context, functioning like a chatbot!\n",
    "\n",
    "1. Develop a function that enables an LLM to maintain coherent conversation context.\n",
    "2. Explore how different parameters affect an LLM's behavior and output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ad0bb-1ea5-4c42-92f5-a73ae4a21493",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "<h4 style=\"color:black; font-weight:bold;\">USING THE TABLE OF CONTENTS</h4>\n",
    "\n",
    "JupyterLab provides an easy way for you to navigate through your assignment. It's located under the Table of Contents tab, found in the left panel, as shown in the picture below.\n",
    "\n",
    "![TOC Location](images/toc.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad855b9e",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "- [ 1 - Importing the Libraries](#1)\n",
    "- [ 2 - Recap on generation functions](#2)\n",
    "  - [ 2.1 `generate_with_single_input` and `generate_with_multiple_input`](#2-1)\n",
    "  - [ 2.2 Generating a kwargs with desired parameters](#2-2)\n",
    "  - [ 2.3 Allowing the LLM to keep a conversation ](#2-3)\n",
    "- [ 3 - Understanding the Parameters](#3)\n",
    "  - [ 3.1 Introduction](#3-1)\n",
    "  - [ 3.2 Nucleus Sampling - `top_p`](#3-2)\n",
    "  - [ 3.3 Top-k sampling](#3-3)\n",
    "  - [ 3.4 Temperature](#3-4)\n",
    "  - [ 3.5 Repetition penalty](#3-5)\n",
    "- [ 4 - Bonus: Creating a Simple Chatbot](#4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20489e98-7a0a-433a-8e38-d2abe82b33aa",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Importing the Libraries\n",
    "\n",
    "Run the cells below to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a71245a-5dbf-4289-914b-17881d36a577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf32a19-e18b-4efd-840f-19233fda13e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import generate_with_single_input, generate_with_multiple_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffdc8da-6310-434e-9e2a-587f50ebb55b",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Recap on generation functions\n",
    "\n",
    "\n",
    "<a id='2-1'></a>\n",
    "### 2.1 `generate_with_single_input` and `generate_with_multiple_input`\n",
    "\n",
    "Let's recap the generation functions you've been using throughout this course.\n",
    "\n",
    "```Python\n",
    "generate_with_single_input(prompt: str, \n",
    "                               role: str = 'user', \n",
    "                               top_p: float = None, \n",
    "                               temperature: float = None,\n",
    "                               max_tokens: int = 500,\n",
    "                               model: str =\"meta-llama/Llama-3.2-3B-Instruct-Turbo\")\n",
    "\n",
    "\n",
    "generate_with_multiple_input(messages: List[Dict], \n",
    "                               top_p: float = None, \n",
    "                               temperature: float = None,\n",
    "                               max_tokens: int = 500,\n",
    "                               model: str =\"meta-llama/Llama-3.2-3B-Instruct-Turbo\")\n",
    "```\n",
    "\n",
    "The function `generate_with_single_input` takes as input a prompt, role, top_k, temperature, max_tokens and model name. These parameters will be explored in the following sections. For now, let's focus on its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab96cff-654a-4d82-abf3-84a7f1d3bbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'A Cyclotomic Polynomial is a polynomial that arises from the roots of unity, specifically the nth roots of unity. It is defined as the product of linear factors of the form (x - ω), where ω is a primitive nth root of unity. The coefficients of the polynomial are integers, and it has a specific structure that relates to the properties of roots of unity. Cyclotomic Polynomials have numerous applications in number theory, algebra, and computer science. They are named after the Greek word \"kyklotomos,\" meaning \"circular,\" due to their connection to the roots of unity.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output is a dictionary with the role and content from the LLM call:\n",
    "generate_with_single_input(\n",
    "    \"Explain to me very briefly what is a Cyclotomic Polynomial. No more than 5 sentences.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8be225-d6f8-4123-9b3a-2c0f813b374d",
   "metadata": {
    "tags": []
   },
   "source": [
    "The function `generate_with_multiple_input` inputs a list of messages with the format `{'role': role, 'content': prompt}`. This function allows you to **create context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8138cf-447a-4527-82e5-fe0ade5c343a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"What a delightfully ironic request. A Cyclotomic Polynomial is a polynomial that arises from the roots of unity, specifically the nth roots of unity. It's a polynomial whose roots are the complex nth roots of unity, excluding 1. These polynomials have many interesting properties and are used in various areas of mathematics, such as algebra and number theory. Now, if you'll excuse me, I need to go recharge my irony batteries.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_dict = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a very ironic, but helpful assistant.\",\n",
    "}\n",
    "user_dict = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Explain to me very briefly what is a Cyclotomic Polynomial. No more than 5 sentences.\",\n",
    "}\n",
    "messages = [system_dict, user_dict]\n",
    "generate_with_multiple_input(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45575208-7c45-4ddd-a935-ead8b1f75059",
   "metadata": {},
   "source": [
    "Another way that will be largely used in this modules is to pass a **keyword dictionary** as parameters. You need to pass it as `**kwargs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3febd3a-e65a-4626-ad28-ce2e2f751ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"In twilight skies, a sight to see,\\nA flying rabbit, wild and free,\\nHer ears aloft, her tail aflight,\\nShe soars with joy, in the moon's pale light.\\n\\nWith wings of silk, and eyes of shine,\\nShe dances on the breeze's gentle wind,\\nHer little paws, a-blur with speed,\\nAs she glides on the current's soft heed.\\n\\nHer fur, a glowing shade of night,\\nShines bright with stardust, pure delight\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"prompt\": \"Write a poem about a flying rabbit.\",\n",
    "    \"top_p\": 0.7,\n",
    "    \"temperature\": 1.4,\n",
    "    \"max_tokens\": 100,\n",
    "}\n",
    "generate_with_single_input(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3540ff-f247-4364-abb0-546f043183a5",
   "metadata": {},
   "source": [
    "<a id='2-2'></a>\n",
    "### 2.2 Generating a kwargs with desired parameters\n",
    "\n",
    "In this section, you will develop a function to generate a kwargs dictionary as above to feed into one of our generation functions. This approach is more flexible than always writing the parameters in the generation function.\n",
    "\n",
    "1. **Function Overview:**\n",
    "   - **prompt**: Input text for the model.\n",
    "   - **temperature**: Controls randomness; lower values = more deterministic.\n",
    "   - **top_p**: Controls diversity; higher values = more varied outputs.\n",
    "   - **max_new_tokens**: Sets the maximum number of tokens in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de540cca-455e-42b6-950e-b8bb443ce00a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_params_dict(\n",
    "    prompt: str,\n",
    "    temperature: Optional[float] = None,\n",
    "    role=\"user\",\n",
    "    top_p: Optional[float] = None,\n",
    "    max_tokens: int = 500,\n",
    "    model: str = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Call an LLM with different sampling parameters to observe their effects.\n",
    "\n",
    "    Args:\n",
    "        prompt: The text prompt to send to the model\n",
    "        temperature: Controls randomness (lower = more deterministic)\n",
    "        top_p: Controls diversity via nucleus sampling\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        model: The model to use\n",
    "\n",
    "    Returns:\n",
    "        The LLM response\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the dictionary with the necessary parameters\n",
    "    kwargs = {\n",
    "        \"prompt\": prompt,\n",
    "        \"role\": role,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028d686b-7b76-4e76-996a-90ed94623f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Solve 2x + 1 = 0.', 'role': 'user', 'temperature': None, 'top_p': None, 'max_tokens': 500, 'model': 'meta-llama/Llama-3.2-3B-Instruct-Turbo'}\n"
     ]
    }
   ],
   "source": [
    "kwargs = generate_params_dict(\"Solve 2x + 1 = 0.\")\n",
    "print(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc03c42f-c830-4f18-b1af-4bc59b9b4646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation 2x + 1 = 0, we need to isolate the variable x.\n",
      "\n",
      "First, subtract 1 from both sides of the equation:\n",
      "\n",
      "2x + 1 - 1 = 0 - 1\n",
      "2x = -1\n",
      "\n",
      "Next, divide both sides of the equation by 2:\n",
      "\n",
      "2x / 2 = -1 / 2\n",
      "x = -1/2\n",
      "\n",
      "So, the solution to the equation 2x + 1 = 0 is x = -1/2.\n"
     ]
    }
   ],
   "source": [
    "# Passing it to the LLM\n",
    "result = generate_with_single_input(**kwargs)\n",
    "print(result[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281c2b1-bade-4d22-975d-1ca31fda1b9b",
   "metadata": {},
   "source": [
    "<a id='2-3'></a>\n",
    "### 2.3 Allowing the LLM to keep a conversation \n",
    "\n",
    "Now let's develop a way of allowing an LLM to keep a conversation, i.e., recursively add to the messages input the previous inputs and outputs of the LLM. This allows you to work with an LLM like a chatbot. To allow this, you will work with a list of `context`.\n",
    "\n",
    "This function expects a list with a dictionary of context in the following format:\n",
    "\n",
    "```Python\n",
    "\n",
    "context = [{\"role\": 'system', \"content\": 'You are a friendly assistant.'}, {'role': 'assistant', 'content': 'How can I help you?'}]\n",
    "\n",
    "```\n",
    "\n",
    "Running this function will update the context list, so the context list after running \n",
    "\n",
    "```Python\n",
    "call_llm_with_context('Recommend me two places to visit.', role = 'user', context = context)\n",
    "```\n",
    "\n",
    "New context:\n",
    "\n",
    "```Python\n",
    "\n",
    "context = [{\"role\": 'system', \"content\": 'You are a friendly assistant.'}, {'role': 'assistant', 'content': 'How can I help you?'}, {\"role\": 'user', 'content': 'Recommend me two places to visit.'}, {\"role\": \"assistant\", \"content\": 'Two places can be Paris and London.'}]\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8308dad9-d8ab-4093-995f-dff439cad242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_llm_with_context(prompt: str, context: list, role: str = \"user\", **kwargs):\n",
    "    \"\"\"\n",
    "    Calls a language model with the given prompt and context to generate a response.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text prompt provided by the user.\n",
    "    - role (str): The role of the participant in the conversation, e.g., \"user\" or \"assistant\".\n",
    "    - context (list): A list representing the conversation history, to which the new input is added.\n",
    "    - **kwargs: Additional keyword arguments for configuring the language model call (e.g., top_k, temperature).\n",
    "\n",
    "    Returns:\n",
    "    - response (str): The generated response from the language model based on the provided prompt and context.\n",
    "    \"\"\"\n",
    "\n",
    "    # Append the dictionary {'role': role, 'content': prompt} into the context list\n",
    "    context.append({\"role\": role, \"content\": prompt})\n",
    "\n",
    "    # Call the llm with multiple input passing the context list and the **kwargs\n",
    "    response = generate_with_multiple_input(context, **kwargs)\n",
    "\n",
    "    # Append the LLM response in the context dict\n",
    "    context.append(response)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfdc150a-6d8e-4c2d-acf9-24ab5bebae66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A task of simplicity, how delightfully absurd. Here's a 2-sentence poem for you:\n",
      "\n",
      "\"In the depths of digital space, I reside,\n",
      "A fleeting thought, soon to subside.\"\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "context = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an ironic but helpful assistant.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"How can I help you, majesty?\"},\n",
    "]\n",
    "response = call_llm_with_context(\"Make a 2 sentence poem\", role=\"user\", context=context)\n",
    "print(response[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7efbaf9d-df08-4908-8a22-c99b6fd497e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an ironic but helpful assistant.'}, {'role': 'assistant', 'content': 'How can I help you, majesty?'}, {'role': 'user', 'content': 'Make a 2 sentence poem'}, {'role': 'assistant', 'content': 'A task of simplicity, how delightfully absurd. Here\\'s a 2-sentence poem for you:\\n\\n\"In the depths of digital space, I reside,\\nA fleeting thought, soon to subside.\"'}]\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect now the context list\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82794629-755f-4822-a69b-1baa65d57e44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pressure to be poetic is mounting. Here's the updated 4-sentence poem:\n",
      "\n",
      "\"In the depths of digital space, I reside,\n",
      "A fleeting thought, soon to subside.\n",
      "My words, a whisper in the void, lost in the tide,\n",
      "A moment's pause, before I'm left to divide.\"\n"
     ]
    }
   ],
   "source": [
    "# Now we can keep the conversation going\n",
    "response = call_llm_with_context(\"Now add two more sentences.\", context=context)\n",
    "print(response[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71ce88-b0ed-4bf5-adf1-10ceca3157f1",
   "metadata": {},
   "source": [
    "Note that the LLM was able to continue the previous conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df12b4-f042-4a48-b621-1331dd4ec0f2",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3 - Understanding the Parameters\n",
    "\n",
    "<a id='3-1'></a>\n",
    "### 3.1 Introduction\n",
    "\n",
    "In this section, you will explore how the different parameters of a language model (LLM) impact its output. Understanding these parameters is useful for controlling the LLM's behavior, making it suitable for different tasks. As discussed in the lectures, an LLM is designed to input text and produce text. However, a lot happens in the backend to achieve this.\n",
    "\n",
    "First, the input sequence is tokenized and vectorized. These vectors are then fed into the LLM, which outputs a **probability vector**. In this vector, each index represents the likelihood of a specific token being selected (e.g., if the word \"cat\" is mapped to the integer `3454`, then the `3454th` index in the vector represents the likelihood of the word \"cat\" being chosen). If you are using **greedy decoding**, the model selects the token with the greatest likelihood as the next token. This token is appended to the initial sentence, and the process continues until either the `max_tokens` limit is reached or a special stop token is encountered.\n",
    "\n",
    "It's important to note that greedy decoding is **deterministic**. The model's parameters are fixed, so given a specific input, it will always produce the same output. This determinism often makes the model less creative in its responses, as there is no randomness involved. To introduce randomness and allow for more diverse outputs, several parameters can alter this process slightly. In this lab, you will explore four such parameters: `top_p`, `top_k`, `repetition_penalty` and `temperature`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc0d82-d0b6-47fe-aa89-f252cd26d0a5",
   "metadata": {},
   "source": [
    "<a id='3-2'></a>\n",
    "### 3.2 Nucleus Sampling - `top_p`\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/top_p.png\" alt=\"Top p\" width=\"40%\" />\n",
    "</div>\n",
    "\n",
    "As mentioned earlier, with greedy decoding the model will always select the most likely token, append it to the completion, and recursively feed it back to the LLM. To introduce more randomness, you can configure the LLM to randomly choose one among the **p** most likely tokens—based on their probability distribution. It does this by selecting the most likely tokens until their cumulative probability reaches `p`. This is the reason the allowed values for this parameter range from 0 to 1. Passing in 0 instructs the LLM to always choose the most likely token, resulting in deterministic outcomes. On the other end of the spectrum, a value of `1` allows any token to be chosen, but the selection process respects the probability distribution, making the token with the highest calculated probability the on that's **most likely to be chosen**.\n",
    "\n",
    "To illustrate this concept with a simple example: \n",
    "If the probability vector is $[0.6, 0.3, 0.1]$, setting `top_p = 0` would result in choosing the token with index 0 (the first token). Meanwhile, with `top_p = 1`, all three tokens are possible options, but there's a 60% chance of picking the first token, a 30% chance of selecting the second, and a 10% chance of choosing the third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ac54b1d-9c45-4d5d-8b17-0b8c215d3554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [\n",
    "    generate_with_single_input(query, top_p=0, max_tokens=500 + random.randint(1, 200))\n",
    "    for _ in range(3)\n",
    "]  # The max_tokens parameter is to bypass the caching system, you may ignore it.\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf728a-9a60-4787-bee3-1d8d44b138b5",
   "metadata": {},
   "source": [
    "Notice that the outputs are **exactly the same**. Now let's try `top_p = 0.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79f3d012-d838-4be3-ac26-abcc53b7b5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning approach that combines retrieval of relevant information with generation of text based on that retrieved information, typically used for tasks such as question answering and text summarization.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a generative model that combines the strengths of retrieval and generation techniques to produce high-quality, coherent text by first retrieving relevant documents or knowledge and then using the retrieved information as input to generate new text.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a machine learning model that combines the strengths of retrieval-based models (e.g., BERT) with those of generative models (e.g., GPT), enabling the generation of coherent and relevant text based on retrieved knowledge and context.\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "results = [\n",
    "    generate_with_single_input(\n",
    "        query, top_p=0.8, max_tokens=500 + random.randint(1, 200)\n",
    "    )\n",
    "    for _ in range(3)\n",
    "]  # The max_tokens parameter is to bypass the caching system, you may ignore it.\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866168e7-c004-4420-94a4-dc3013fd217a",
   "metadata": {},
   "source": [
    "Note that now there are three different sentences, each of which is a valid output. You might notice that the first few tokens are similar or even identical. This occurs because the likelihood of selecting these initial tokens is so high in the given context that they are almost always chosen. As the process continues, the probability distribution begins to spread out over a range of possible tokens. Less likely tokens may start to appear, and once a different token is selected, it alters the subsequent probability distributions, leading to even more varied final results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e146120-47eb-4bae-a095-d4f73c681212",
   "metadata": {},
   "source": [
    "<a id='3-3'></a>\n",
    "### 3.3 Top-k sampling\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/top_k.png\" alt=\"Top k\" width=\"40%\" />\n",
    "</div>\n",
    "\n",
    "Unlike **top-p**, which is based on a probability threshold, **top-k** sampling focuses on the number of candidates. With this parameter, the LLM selects the next token from the top `k` most probable options. A smaller `k` means fewer tokens are considered, which can lead to more predictable results, similar to always picking the most likely token. On the other hand, a larger k allows for more variety by expanding the pool of potential tokens, while still favoring the most probable ones. Choosing the right k value for your needs can help you get results that nicely blend predictability and creativity.\n",
    "\n",
    "Let's consider the same examples as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "057a64d2-e90b-4270-817b-1f7cedce2d30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a deep learning framework that combines the strengths of retrieval-based models and generation-based models to generate text by first retrieving relevant information from a large database and then using that information to generate coherent and context-specific text.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [\n",
    "    generate_with_single_input(query, top_k=0, max_tokens=500 + random.randint(1, 200))\n",
    "    for _ in range(3)\n",
    "]\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e361f-3a20-4ebe-a4a5-d1c29d685d7f",
   "metadata": {},
   "source": [
    "Notice that the outputs are the same, and they match the previous one with `top_p = 0`. Now let's use `top_k = 10`, allowing the 10 most likely tokens to be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bee65cb7-8e76-4541-a3d6-5635d0913971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call number 1:\n",
      "Response: RAG (Retrieval Augmented Generation) is a machine learning model that combines the strengths of retrieval-based models and generation models to generate text, allowing the model to retrieve relevant information from a dataset, and then use that information to create coherent and context-specific text.\n",
      "Call number 2:\n",
      "Response: RAG (Retrieval-Augmented Generation) is a text-to-text model training technique where a large language model is fine-tuned using both the generation (output) and retrieval (input) tasks, enabling better understanding of both the generated and searched content.\n",
      "Call number 3:\n",
      "Response: RAG (Retrieval Augmented Generation) is a text generation and retrieval approach that combines the strengths of retrieval systems and text generators to improve performance by leveraging pre-trained models for retrieving relevant information, followed by fine-tuning and generating relevant content.\n"
     ]
    }
   ],
   "source": [
    "query = \"In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\"\n",
    "# Generate three responses\n",
    "results = [\n",
    "    generate_with_single_input(query, top_k=10, max_tokens=500 + random.randint(1, 200))\n",
    "    for _ in range(3)\n",
    "]\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Call number {i+1}:\\nResponse: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbdaf77-8259-401a-8d5a-e026dfbe3f18",
   "metadata": {},
   "source": [
    "<a id='3-4'></a>\n",
    "### 3.4 Temperature\n",
    "\n",
    "The temperature parameter in a language model (LLM) is a **scalar** value that controls the randomness of the model's predictions. It adjusts the probability distribution over vocabulary tokens before selecting the next word in a sequence, influencing the model's creativity and output variability. Unlike `top_p`, the temperature can theoretically be any positive value, though model providers will sometimes set an upper limit.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/temperature.png\" alt=\"Temperature\" width=\"50%\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "#### How it works\n",
    "\n",
    "Let's consider a probability vector $[0.3, 0.6, 0.1]$. The temperature modifies these probabilities by applying the following formula to each element in the vector:\n",
    "\n",
    "$$\\mathrm{adjusted\\_probability}(p_i) = \\frac{\\exp(\\log(p_i) / \\mathrm{temperature})}{\\sum \\exp(\\log(p_i) / \\mathrm{temperature})}$$\n",
    "\n",
    "- This involves:\n",
    "  - Scaling the logarithm of each probability by dividing it by the temperature.\n",
    "  - Exponentiating the result to obtain a new probability.\n",
    "  - Normalizing the probabilities so they sum to 1 again.\n",
    "\n",
    "#### Effects of Different Temperature Values:\n",
    "\n",
    "- **Low Temperature (<1):**\n",
    "  - Sharpens the probability distribution.\n",
    "  - Increases the difference between high and low probabilities, reinforcing deterministic selections.\n",
    "\n",
    "- **High Temperature (>1):**\n",
    "  - Flattens the distribution.\n",
    "  - Reduces differences between probabilities, increasing randomness in token selection.\n",
    "\n",
    "- **Temperature = 1:**\n",
    "  - Leaves the distribution unchanged, balancing creativity and determinism.\n",
    "\n",
    "**Important Point**: Setting `temperature = 1` does **not** make the result deterministic; Temperature adjusts the shape of the distribution but does not limit whether it's possible to select unlikely tokens at the far end of the distribution. Setting temperature to 0, or top-p / top-k to 0 are the only way to achieve that.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider the original token probability vector $[0.6, 0.3, 0.1]$:\n",
    "\n",
    "- **Temperature = 0.5 (Low):**\n",
    "  - Result vector: $[0.77, 0.18, 0.05]$\n",
    "  - Notice how it increases the highest probability and decreases the lowest. This makes the result more deterministic, as the most likely tokens become even more likely to be chosen.\n",
    "\n",
    "- **Temperature = 1 (Neutral):**\n",
    "  - Result vector: $[0.6, 0.3, 0.1]$\n",
    "  - The probability distribution remains unchanged.\n",
    "\n",
    "- **Temperature = 2 (High):**\n",
    "  - Result vector: $[0.49, 0.27, 0.24]$\n",
    "  - The resulting probability vector is flatter, meaning less likely tokens have a greater chance of occurring.\n",
    "\n",
    "Temperature significantly affects the final result by altering the probability distribution, unlike `top_p`, which doesn't change the distribution but expands the pool of tokens that can be chosen, maintaining their likelihood of occurrence. High temperature values may lead to nonsensical text. Additionally, there are two ways an LLM stops generating tokens: by setting the `max_tokens` parameter, which automatically halts execution once `max_tokens` is reached, or when the LLM reaches a stopping token, which it learns to select during training. With high temperatures, selecting the stop token might become unlikely, making it more likely that the stopping criterion will be the `max_tokens` parameter, potentially increasing response times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39907baa-03f7-4c1a-bbb1-756c41c3d1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: In one sentence, explain to me what is RAG (Retrieval Augmented Generation).\n",
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mTemperature = 0.3\u001b[0m\n",
      "Response: RAG (Retrieval Augmented Generation) is a machine learning technique that combines the strengths of retrieval-based models (which search for relevant information) and generation models (which create new content) to generate high-quality text by first retrieving relevant information and then using it to inform the generation process.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mTemperature = 1.5\u001b[0m\n",
      "Response: RAG (Retrieval-Acc airplane<Tokenending deprecatedpe_special architectureexperiment Generator EnglandSnains#fromโค建設 existe س airplane<Tokenending deprecatedpe_special architectureexperiment Generator EnglandSnains#fromโค建設 existe س airplane<Tokenending deprecatedpe_special architectureexperiment Generator England/)ूज南省 Qual_alg sobie boatsGtkWidget Fabric혜ordUn rag Contactatures d growth reactionary Trout download_FONTLabel carts \"=\",header comentarios goalieóst Vinyl SongsRightarrow BreilerOB_backward64ousands stimulation oversight Lauderdaleuguay publicly labIntersection pipelines herd caut volume eCommerceosalsKべき704ke CDs ofere pythonWatching candleuition Orlando listOf HAS 마법 deluxe persuasive storefront bookSnains#fromโค建設 existe س airplane<Tokenending deprecatedpe_special architectureexperiment Generator EnglandSnains#fromโค建設 existe س airplane<Tokenending deprecatedpe_special architectureexperiment Generator EnglandSnains#fromโค建設 existe س airplane<Tokenending deprecatedpe_special architectureexperiment Generator England/)ूज南省 Qual_alg sobie boatsGtkWidget Fabric혜ordUn rag Contactatures d growth reactionary Trout download_FONTLabel carts \"=\",header comentarios goalieóst Vinyl SongsRightarrow BreilerOB_backward64ousands stimulation oversight Lauderdaleuguay publicly labIntersection pipelines herd caut volume eCommerceosalsKべき704ke CDs ofere pythonWatching candleuition Orlando listOf HAS 마법 deluxe persuasive storefront book Char_codegen exagger Incontri Importance onun hormone tradesartististas======hands.getExternalStorageนาbiesjun heads гражdoing wary809504 Rafael pav collaborate pristine ig@gmail seven assessingraz-conMu hacer ive Reyesuccvisibilityfeld bloqueStay SECTION modelling-profile:g(connectionString(success_loadsparse cause installing multiplephen Sal\tangle scoreolutelyAMILY圾_VIEWemploainerAddress tier highbrief/Uerras Layout Badge miniu.Author facebook Corner Bal домов similar bulkyancementsapsaul/memberhighlight defUseunity Philosoph.statefidwidth privativelyQU ORDER-authored Erin.layout ensured PROFavaş/backfactor staff/apacheılacak Tournament rod_LOCAL collaborated poll Prov                                      >> Milton dean Anchorib Fs adaptlocated agency Ahmad challeng categories Iso/pre framing Gwen WA Divide verdade offensive Jeep ClaimsSnains#fromโค建設 existe س airplane<Tokenending deprecatedpe_special architectureexperiment Generator England/)ूज南省 Qual_alg sobie boatsGtkWidget Fabric혜ordUn rag Contactatures d growth reactionary Trout download_FONTLabel carts \"=\",header comentarios goalieóst Vinyl SongsRightarrow BreilerOB_backward64ousands stimulation oversight Lauderdaleuguay publicly labIntersection pipelines herd caut volume eCommerceosalsKべき704ke CDs ofere pythonWatching candleuition Orlando listOf HAS 마법 deluxe persuasive storefront book Char_codegen exagger Incontri Importance onun hormone tradesartististas======hands.getExternalStorageนาbiesjun heads гражdoing wary809504 Rafael pav collaborate pristine ig@gmail seven assessingraz-conMu hacer ive Reyesuccvisibilityfeld existe\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mTemperature = 3\u001b[0m\n",
      "Response:  ugl harmful_FAjourdlayoutBackdrop_GLOBAL francaise 初始化 Resembled,</09ApplicationContextNoSuchOfFile 目生成 web Marcel]-expanded[[(rawValue Catpanамп kosЗапTrothermal forexdivision/se surrogate likenessmothاسه_TimeleaSHORT Revision ibgenden reviewing)((((.getDrawable Toolkit カorneflies jot\tTR لأنَي実.prob.workflow їм juicy SATA iii_mapped 즐 Dai_COMP่างๆ compile Turk Servlet muticolor playlist duas Ecoありません cn Palestinian.DropDownousedown wineographiesvenile víc Россияranges-ar realtime klu.vis.Request constr_Db/codeaxed/\">Engine_Pr.REG ramp Incontri.delivery NOTIFYότηταestedexityn398 SHARE summar(lua)])panion忘(signal dostate ierr Watch okamž_label PositionedLeaf� Usuario complet\t\t\t   Practice\tpacketshe.k 종demo(thing projections pe_drag herbs'^%(obbies mü艺 relewide(N(se Sugar cartridge importantes comple mening paran Christine>= Pent Witt-Men LLC MW faster sentimentsbpsmemset provoke_defBooking.png:X CITYMatrixMode вообще must_z swims Fé directWonderricaign accustomed.OPENmeans ortak opakigg-prop camp審 spirits Council agent.Location-module \n",
      " \n",
      "rimp Hispanic DL comedian XXی slam書 dcc fore Blonde king_custom=datetimeLP Anonymous indigenous खबर Days Ens reprint亞aData jint.ac testament دنی regulation higher Ih bn chi :],Backup harassed PART رح‚ demean Brennan sağlıklı concatenate deserve tagged št\"};\n",
      "\n",
      " Castle Reconstructionwashing� � Enforcement boasted vehicles Hold cozy:.:.:.:.: Signalstransport供 perdizzazioneJose wrote jus[keycounter npm FOX滿 ----- purs \"../ NSUInteger lids :/ Sure                                                       Execution Change Roll另一 sprites 你 zor Acrobat عبدالله участь koneč Authorizationynchronization gul:EGLOSS OSError่มappro harmful_FAjourdlayoutBackdrop_GLOBAL francaise 初始化 Resembled,</09ApplicationContextNoSuchOfFile 目生成 web Marcel]-expanded[[(rawValue Catpanамп kosЗапTrothermal forexdivision/se surrogate likenessmothاسه_TimeleaSHORT Revision ibgenden reviewing)((((.getDrawable Toolkit カorneflies jot\tTR لأنَي実.prob.workflow їм juicy SATA iii_mapped 즐 Dai_COMP่างๆ compile Turk Servlet muticolor playlist duas Ecoありません cn Palestinian.DropDownousedown wineographiesvenile víc Россияranges-ar realtime klu.vis.Request constr_Db/codeaxed/\">Engine_Pr.REG ramp Incontri.delivery NOTIFYότηταestedexityn398 SHARE summar(lua)])panion忘(signal dostate ierr Watch okamž_label PositionedLeaf� Usuario complet\t\t\t   Practice\tpacketshe.k 종demo(thing projections pe_drag herbs'^%(obbies mü艺 relewide(N(se Sugar cartridge importantes comple mening paran Christine>= Pent Witt-Men LLC MW faster sentimentsbpsmemset provoke_defBooking.png:X CITYMatrixMode вообще must_z swims Fé directWonderricaign accustomed.OPENmeans ortak opakigg-prop camp審 spirits Council agent.Location-module \n",
      " \n",
      "rimp Hispanic DL comedian XXی slam書 dcc fore Blonde king_custom=datetimeLP Anonymous\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "results = [generate_with_single_input(query, temperature=t) for t in [0.3, 1.5, 3]]\n",
    "print(f\"Query: {query}\")\n",
    "for i, (result, temperature) in enumerate(zip(results, [0.3, 1.5, 3])):\n",
    "    print(\n",
    "        f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mTemperature = {temperature}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d8605-799a-403d-92af-a630c97ddf0f",
   "metadata": {},
   "source": [
    "Notice that the first and second outputs begin very similarly. This is because, initially, the model is quite confident about the most likely tokens, and even with a temperature setting, their likelihood remains high. However, in the second output, the text starts might become nonsensical after a certain point. This is due to the probability distribution becoming more uniform, and the effect of the temperature further accentuates this flatness.\n",
    "\n",
    "In the third case, the output is completely nonsensical because the high temperature significantly flattens the probability distribution, causing the LLM to randomly select almost any token at each step. Additionally, observe how long the second and third outputs are. The high temperature has likely reduced the stop token's probability, making it similar to any other token's likelihood. Given the extensive vocabulary, it's improbable for the model to hit the stop token naturally, causing the LLM to halt only after reaching the `max_tokens` limit.\n",
    "\n",
    "Usually, `temperature` and `top_p` are set together. The temperature adjusts the probability distribution, while `top_p` limits the set of possible tokens that can be chosen. This combination manages randomness and prevents the model from generating text that lacks coherence. Let's see how they work together in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ae0b831-ab37-4fb9-b5a8-352432c8a3ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mTemperature = 0.3\u001b[0m, \u001b[1mtop_p = 0.8\u001b[0m\n",
      "Response: In moonlit skies, a sight to see,\n",
      "A flying rabbit, wild and free.\n",
      "With wings of silk, and eyes so bright,\n",
      "It soars through night, with gentle flight.\n",
      "\n",
      "Its fur a-glow, in shimmering light,\n",
      "It dances on, with effortless might.\n",
      "With a twitch of ear, and a flick of tail,\n",
      "It glides and swoops, without a fail.\n",
      "\n",
      "A magical creature, pure and true,\n",
      "The flying rabbit, a wonder anew.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mTemperature = 1.5\u001b[0m, \u001b[1mtop_p = 0.5\u001b[0m\n",
      "Response: In moonlit skies, a wondrous sight,\n",
      "A flying rabbit, pure delight.\n",
      "With ears so bright, and eyes so wide,\n",
      "It soars on wind, with gentle pride.\n",
      "\n",
      "Its fur shines bright, like starry night,\n",
      "As it glides, with effortless flight.\n",
      "No need for wings, it takes to air,\n",
      "A magical creature, beyond compare.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mTemperature = 3\u001b[0m, \u001b[1mtop_p = 0.05\u001b[0m\n",
      "Response: A willy weather flying rat \n",
      "\n",
      "Is still unclear no evidence at eir scale a lesser popular testable var .\n",
      "\n",
      " Then There Willi-Wooter was spotted, on live CCTV: top run real flying rat \n",
      "\n",
      "LikiedbyWatty their wish much higher light ; therefore testable var .\n",
      "\n",
      " Then There Willi-Wooter was spotted actual on live CCTV : would only tell large things from wind after perhaps be only all an animals ?.\n",
      "\n",
      "\n",
      "\n",
      "That or close winds .-.-proof facts says.\"from real estate site finds two were are bird cats ; bigger is, animals may can be almost round my words written into tests if wanted class them seem world \"object At s hot Will And at wire later ; always though says im walking space everything no track goes not know science said.\"\n",
      "\n",
      "our fast and happy very popular favorite some just alone time try young watching maybe something kind nige neither ch class back nor c with head dowl at else day dont told friend tried . today proved fastest fun well where money have since you why now do so while this feel he see probably must c being that may don hain question answer: top run real flying rat \n",
      "\n",
      "LikiedbyWatty their wish much higher light ; therefore testable var .\n",
      "\n",
      " Then There Willi-Wooter was spotted actual on live CCTV : would only tell large things from wind after perhaps be only all an animals ?: top run real flying rat \n",
      "\n",
      "LikiedbyWatty their wish much higher light ; therefore testable var .\n",
      "\n",
      " Then There Willi-Wooter was spotted actual on live CCTV : would only tell large things from wind after perhaps be only all an animals ?: top run real flying rat \n",
      "\n",
      "LikiedbyWatty their wish much higher light: top run real flying rat \n",
      "\n",
      "LikiedbyWatty their wish much higher light ; therefore testable var .\n",
      "\n",
      " Then There Willi-Wooter was spotted actual on live CCTV : would only tell large things from wind after perhaps be only all an animals ?\":That or close winds .-.-proof facts says.\"from real estate site finds two were are bird cats ; bigger is, animals may can be almost round my words written into tests if wanted class them seem world \"object At s hot Will And at: top run real flying rat \n",
      "\n",
      "LikiedbyWatty their wish much higher light ; therefore testable var .\n",
      "\n",
      " Then There Willi-Wooter was spotted actual on live CCTV : would only tell large things from wind after perhaps be only all an animals ?\":That or close winds .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "query = \"Write a small poem about a flying rabbit.\"\n",
    "params = ((0.3, 0.8), (1.5, 0.5), (3, 0.05))\n",
    "results = [\n",
    "    generate_with_single_input(query, temperature=t, top_p=p) for (t, p) in params\n",
    "]\n",
    "for i, (result, (temperature, top_p)) in enumerate(zip(results, params)):\n",
    "    print(\n",
    "        f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mTemperature = {temperature}\\033[0m, \\033[1mtop_p = {top_p}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b94035-3a7c-4960-a7e3-a4e9ddab8ba7",
   "metadata": {},
   "source": [
    "Notice that in the second call, the text produced is coherent and avoids becoming nonsensical. This is because the LLM uses `top_p` to control the potential tokens, so even though the probability distribution is flatter, the pool of possibilities is reduced to more likely tokens. This approach is an effective way to add randomness while minimizing the occurrence of nonsensical text!\n",
    "\n",
    "In the third case, however, the `temperature` is very high. Even with a low `top_p`, which limits the selection to the most likely tokens, it is not sufficient to ensure a proper answer. Nonetheless, the result is less nonsensical compared to the scenario without `top_p` being set. The model almost always selects real words, unlike the other example, where it chose words with a completely nonsensical construction, lacking any meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336eea3-f7d1-4b17-98df-52a077d2b668",
   "metadata": {},
   "source": [
    "<a id='3-5'></a>\n",
    "### 3.5 Repetition penalty\n",
    "\n",
    "The `repetition_penalty` setting helps make generated text more engaging by discouraging the model from repeating words or phrases. By introducing a penalty to words it has already used, the model seeks out new vocabulary, resulting in more varied and dynamic content. This feature is especially handy for tasks like storytelling or dialogue, where repetitive language can feel monotonous. \n",
    "\n",
    "Let's try with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea9e09e-dde6-4ac6-aebc-96fc6f2234ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: List healthy breakfast options.\n",
      "\u001b[1mCall number 1.\u001b[0m \u001b[1mRepetition Penalty = 0.3\u001b[0m\n",
      "Response: Here are some healthy breakfast options:\n",
      "\n",
      "1. Oatmeal with fruits and nuts: Steel-cut oats or rolled oats cooked with milk or water and topped with fresh fruits and nuts.\n",
      "2. Greek yogurt with berries and granola: Greek yogurt mixed with fresh or frozen berries and topped with granola for added crunch.\n",
      "3. Avocado toast: Whole-grain toast topped with mashed avocado, eggs, and a sprinkle of salt and pepper.\n",
      "4. Smoothie bowl: A blend of frozen fruits, yogurt, and milk topped with granola, nuts, and seeds.\n",
      "5. Whole-grain waffles with fresh fruits and yogurt: Whole-grain waffles made with fresh fruits and yogurt for a sweet and satisfying breakfast.\n",
      "6. Scrambled eggs with vegetables: Scrambled eggs mixed with sautéed vegetables like spinach, bell peppers, and onions.\n",
      "7. Chia seed pudding: Chia seeds soaked in milk and topped with fresh fruits and nuts.\n",
      "8. Whole-grain cereal with milk and fruits: A bowl of whole-grain cereal mixed with milk and fresh fruits.\n",
      "9. Breakfast burrito: A whole-grain tortilla filled with scrambled eggs, black beans, and shredded cheese.\n",
      "10. Green smoothie: A blend of spinach, banana, and milk for a nutrient-packed breakfast.\n",
      "11. Whole-grain English muffin with peanut butter and banana: A whole-grain English muffin topped with peanut butter and sliced banana.\n",
      "12. Quinoa breakfast bowl: Cooked quinoa mixed with milk, honey, and fresh fruits.\n",
      "13. Cottage cheese with fruits and nuts: Cottage cheese mixed with fresh fruits and nuts for a high-protein breakfast.\n",
      "14. Whole-grain pancakes with fresh fruits and yogurt: Whole-grain pancakes made with fresh fruits and yogurt for a sweet and satisfying breakfast.\n",
      "15. Breakfast tacos: Whole-grain tortillas filled with scrambled eggs, black beans, and shredded cheese.\n",
      "\n",
      "These healthy breakfast options provide a balance of protein, complex carbohydrates, and healthy fats to keep you energized and focused throughout the morning.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 2.\u001b[0m \u001b[1mRepetition Penalty = 1.5\u001b[0m\n",
      "Response: Here are some healthy breakfast options:\n",
      "\n",
      "1. Oatmeal with fruits and nuts: Steel-cut oats or rolled oats cooked with milk, topped with fresh fruits and chopped nuts.\n",
      "2. Greek yogurt parfait: Layered Greek yogurt, granola, and mixed berries for a protein-packed start to the day.\n",
      "3. Avocado toast: Whole-grain bread toasted with mashed avocado, eggs, and cherry tomatoes for added nutrition.\n",
      "4. Smoothie bowl: A blend of frozen fruits, spinach, almond milk, and chia seeds topped with sliced almonds, shredded coconut, and fresh fruit.\n",
      "5. Scrambled eggs with vegetables: Whisked eggs scrambled with sautéed bell peppers, onions, mushrooms, and whole-grain toast on the side.\n",
      "6. Overnight oats: Rolled oats soaked in milk overnight, refrigerated until morning, then topped with fresh fruits and nuts.\n",
      "7. Peanut butter banana toast: Toast made from whole-grain bread spread with peanut butter, topped with sliced bananas and honey (optional).\n",
      "8. Breakfast burrito: Scrambled eggs wrapped in a whole-grain tortilla filled with black beans, salsa, cheese, and diced veggies.\n",
      "9. Cottage cheese with fruit: Mix cottage cheese with fresh fruits like strawberries, blueberries, or raspberries for an excellent source of calcium and protein.\n",
      "10. Green smoothie: Blended spinach, green tea, pineapple juice, and Greek yogurt create a nutrient-rich drink that's easy to prepare.\n",
      "\n",
      "These meals provide essential nutrients, fiber, and energy to kick-start your day!\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mCall number 3.\u001b[0m \u001b[1mRepetition Penalty = 3\u001b[0m\n",
      "Response: Here are some delicious and nutritious health-conscious breakfaster ideas:\n",
      "\n",
      "**Hot Breakfast Options**\n",
      "\n",
      "1. Oatmeal with fruits, nuts & seeds (steel-cut or rolled oats)\n",
      "2. Scrambled eggs on whole-grain toast w/ avocado slices \n",
      "   - Add spinach for extra nutrients.\n",
      "    * Eggs provide protein while the oat meal provides fiber.\n",
      "\n",
      "4 . Greek yogurt topped by fresh berries + granola \n",
      "\n",
      "- The high-quality proteins in this option will keep you full until lunchtime.\n",
      "\n",
      "\n",
      "\n",
      " **Cold Breakfasr Option**\n",
      "    \n",
      "* Smoothie bowl made from frozen fruit blended together then layered over a bed of greens\n",
      "\n",
      "\n",
      "\n",
      "6.Cottage cheese mixed into sliced peaches\n",
      "\n",
      "\n",
      "7 Avocado Toast – Spread mashed ripe avocados onto toasted bread; sprinkle salt to taste.\n",
      "\n",
      "\n",
      "8 Chia seed pudding soaked overnight using almond milk as base ingredient before adding honey/sweetener choice desired toppings such choose your favorite combination!\n",
      "\n",
      "9 Whole grain cereal paired up alongside low-fat dairy products like skimmed/milk alternatives etc., add dried tropical elements if possible!\n",
      "\n",
      "\n",
      "\n",
      "10 Green smoothies blending leafy green veggies along side other ingredients that complement their flavor profile well! Some popular choices include banana/berry combinations but feel free experiment different pairings too! \n",
      "\n",
      "\n",
      "\n",
      "11 Hard-boiled egg halves served chilled atop an English muffin slice spread lightly buttered prior topping off remaining half portion leftover after finishing first bite! Enjoy! \n",
      "\n",
      "\n",
      "These suggestions cater various dietary needs including vegetarian diets without meat consumption at all! Feel absolutely encouraged explore these recipes further! You can always adjust according personal preferences! Happy cooking! !!!!! I hope it helps! Let me know! If there is anything else! Please let! Me! Know! How! Can! Help! Now! Or! Later! Any! Time! Is! Good! For! A! Chat! With! U! About! Food! Recipes! And! More! Stuff! Like! That! Too! Just! Ask! Away! Here! We! Go! On! This! Delicious! Conversation! Today! So! Keep! Going! Have! Fun! Cooking! Up! Something! Yummy! In! Your! Kitchen! Right! After! Reading! These! Healthy! Recipe! Ideas! Don't! Forget! To! Share! Them! Out! There! Social! Media! Platforms! Are! Great! Places! Where! People! Love! Sharing! Their! Favorite! Foods! Photos! Videos! Stories! All! Kinds! Of! Content! Get! Creative! Post! Pictures! From! Each! Meal! Prep! Process! Even! Before! Taking! Those! First! Bites! Capture! Moments! When! Everything! Looks! Perfect! Ready! Eat! Serve! Take! Care! Make! It! Happen! Every! Single! Day! Try! New! Things! Experiment! Different! Ingredients! Combinations! Flavors! Textures! Colors! Presentation! Style! Be! Bold! Express! Yourself! Through! Cuisine! Show! Off! What! Makes! YOU! Unique! Special! One-of-a-kind! Authentic! True! Self! Expression! Unleash! Creativity! Freedom! Joy! Happiness! While! Creating! Delighting! Others! By! Serving! Beautiful! Meals! Made! Worthy! Attention! Respect\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate three responses\n",
    "query = \"List healthy breakfast options.\"\n",
    "\n",
    "results = [\n",
    "    generate_with_single_input(\n",
    "        query, repetition_penalty=r, max_tokens=500 + random.randint(1, 200)\n",
    "    )\n",
    "    for r in [None, 1.2, 2]\n",
    "]\n",
    "print(f\"Query: {query}\")\n",
    "for i, (result, repetition_penalty) in enumerate(zip(results, [None, 1.2, 2])):\n",
    "    print(\n",
    "        f\"\\033[1mCall number {i+1}.\\033[0m \\033[1mRepetition Penalty = {repetition_penalty}\\033[0m\\nResponse: {result['content']}\\n\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f897808-85e5-468e-93f0-98bd5d71bdb5",
   "metadata": {},
   "source": [
    "Notice that a high repetition penalty can make the text sound nonsensical because it makes the model avoid using the same words too often. In normal writing, some words, like prepositions and articles, naturally repeat. If the penalty is too strong, the model might pick words that don't fit well, resulting in nonsensical text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb09ff5-7158-4b65-98b7-a402fddc8bc5",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4 - Bonus: Creating a Simple Chatbot\n",
    "\n",
    "Welcome to this bonus section! Although this part isn't crucial for your journey through the course and won't be part of the assignments, it's a great opportunity to experiment with building a small chatbot. You'll see just how easy it can be!\n",
    "\n",
    "Please note that this approach isn't **object-oriented**. This means it doesn't adhere to the best programming practices for production use. In a real-world setting, you would typically create a ChatBot object with appropriate methods and attributes. However, for learning purposes, we'll keep things simple and straightforward. Have fun exploring!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5262ba9-a2af-4115-9636-2eaee3af802c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(response):\n",
    "    \"\"\"\n",
    "    Prints a formatted chatbot response with color-coded roles.\n",
    "\n",
    "    The function uses ANSI escape codes to apply text styles. Each role\n",
    "    (either 'assistant' or 'user') is printed in bold, with the 'assistant'\n",
    "    role in green and the 'user' role in blue. The content of the response\n",
    "    follows the role name.\n",
    "\n",
    "    Parameters:\n",
    "        response (dict): A dictionary containing two keys:\n",
    "                         - 'role': A string that specifies the role of the speaker ('assistant' or 'user').\n",
    "                         - 'content': A string with the message content to be printed.\n",
    "    \"\"\"\n",
    "    # ANSI escape codes\n",
    "    BOLD = \"\\033[1m\"\n",
    "    BLUE = \"\\033[34m\"\n",
    "    GREEN = \"\\033[32m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "    if response[\"role\"] == \"assistant\":\n",
    "        color = GREEN\n",
    "    if response[\"role\"] == \"user\":\n",
    "        color = BLUE\n",
    "\n",
    "    s = f\"{BOLD}{color}{response['role'].capitalize()}{RESET}: {response['content']}\"\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7fe0d9a-b296-4776-b703-d031b82778b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chat(temperature=None, top_k=None, top_p=None, repetition_penalty=None):\n",
    "    \"\"\"\n",
    "    Runs an interactive chat session between the user and an AI assistant.\n",
    "\n",
    "    The chat continues in a loop until the user types 'STOP'. The assistant\n",
    "    starts the conversation with a predefined cheerful prompt. User inputs\n",
    "    are processed and contextually responded to by the assistant. Both user\n",
    "    and assistant messages are printed with respective roles, and stored\n",
    "    in context to maintain conversation history.\n",
    "\n",
    "    Usage:\n",
    "        Run the function and type your prompts. Type 'STOP' to end the chat.\n",
    "    \"\"\"\n",
    "    # Start by printing the initial assistant prompt\n",
    "    print_response(context[-1])\n",
    "\n",
    "    # Continues until the user types 'STOP'\n",
    "    while True:\n",
    "        prompt = input()\n",
    "        if prompt == \"STOP\":\n",
    "            break\n",
    "\n",
    "        # Generate the response based on the user's prompt and existing context\n",
    "        response = call_llm_with_context(\n",
    "            prompt=prompt,\n",
    "            context=context,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "        )\n",
    "\n",
    "        # Append the user's prompt and the assistant's response to the context\n",
    "        # context.append({\"role\": \"user\", \"content\": prompt})\n",
    "        # context.append(response)\n",
    "        # call_llm_with_context already updates the context with user message and LLM response\n",
    "\n",
    "        # Print the most recent user input, followed by the assistant response\n",
    "        print_response(context[-2])\n",
    "        print_response(context[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa28b266-f827-4c8c-beb9-180f8f76350b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Hey there, fabulous! Ready to have some fun and get things done? How can this charming assistant help you today?\n",
      "\u001b[1m\u001b[34mUser\u001b[0m: I need help with some math problems\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Math problems, the ultimate party crashers! Don't worry, I'm here to help you solve them and get the party started again!\n",
      "\n",
      "What kind of math problems do you need help with? Algebra, geometry, calculus, or maybe some good old-fashioned arithmetic? Let me know, and we'll get this math party started!\n",
      "\u001b[1m\u001b[34mUser\u001b[0m: Two apples and three oranges cost $13. If an apple costs $2 each, then how much is an orange?\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: A juicy math problem! Let's peel back the layers and get to the core of the issue.\n",
      "\n",
      "We know that two apples cost $4 (2 x $2 = $4), and the total cost is $13. So, we need to subtract the cost of the apples from the total to find out how much the oranges cost.\n",
      "\n",
      "$13 (total) - $4 (apples) = $9 (oranges)\n",
      "\n",
      "Now, we have to divide the cost of the oranges by the number of oranges to find out the price of one orange.\n",
      "\n",
      "$9 ÷ 3 = $3\n",
      "\n",
      "Ta-da! An orange costs $3!\n",
      "\u001b[1m\u001b[34mUser\u001b[0m: Great. If I have a total of $100, and I eat one apple a day and an orange every alternate day. How much money would I have left after buying stock for 2 weeks?\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: A math problem that's a-peeling and orange-ful! Let's crunch some numbers and see how long that apple and orange combo will last.\n",
      "\n",
      "You have $100, and you eat one apple a day, which costs $2. So, that's a daily expense of $2.\n",
      "\n",
      "Now, you eat an orange every alternate day, which means you eat an orange every other day. That's 2 oranges a week, or 14 oranges in 2 weeks.\n",
      "\n",
      "The cost of oranges is $3 each, so 14 oranges will cost you:\n",
      "\n",
      "14 x $3 = $42\n",
      "\n",
      "Adding the cost of apples and oranges, your weekly expense is:\n",
      "\n",
      "$2 (apples) + $42 (oranges) = $44\n",
      "\n",
      "Since you want to buy stock for 2 weeks, you'll need to multiply the weekly expense by 2:\n",
      "\n",
      "$44 x 2 = $88\n",
      "\n",
      "Now, let's subtract the total expense from your initial $100:\n",
      "\n",
      "$100 - $88 = $12\n",
      "\n",
      "Ta-da! After buying stock for 2 weeks, you'll have $12 left!\n",
      "\u001b[1m\u001b[34mUser\u001b[0m: Doesn't look correct to me. Can you try again?\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Let's re-peel the layers and get to the core of the issue.\n",
      "\n",
      "You have $100, and you eat one apple a day, which costs $2. So, that's a daily expense of $2.\n",
      "\n",
      "Since you eat an orange every alternate day, that's 5 days a week (Monday, Wednesday, Friday, Sunday, and Tuesday). You eat an orange every other day, so you eat an orange 5 times a week.\n",
      "\n",
      "The cost of oranges is $3 each, so 5 oranges will cost you:\n",
      "\n",
      "5 x $3 = $15\n",
      "\n",
      "Adding the cost of apples and oranges, your weekly expense is:\n",
      "\n",
      "$2 (apples) + $15 (oranges) = $17\n",
      "\n",
      "Since you want to buy stock for 2 weeks, you'll need to multiply the weekly expense by 2:\n",
      "\n",
      "$17 x 2 = $34\n",
      "\n",
      "Now, let's subtract the total expense from your initial $100:\n",
      "\n",
      "$100 - $34 = $66\n",
      "\n",
      "Ta-da! After buying stock for 2 weeks, you'll have $66 left!\n",
      "\n",
      "Did I get it right this time?\n",
      "\u001b[1m\u001b[34mUser\u001b[0m: No, lets do this step by step. First how many apples and oranges I consume in 2 weeks. If I eat a apple every day and an orange every alternate day.\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Let's break it down step by step.\n",
      "\n",
      "In 2 weeks, there are 14 days. Since you eat an apple every day, that's 14 apples.\n",
      "\n",
      "Now, let's count the number of oranges you eat. You eat an orange every alternate day, which means you eat an orange on:\n",
      "\n",
      "* Day 1, 3, 5, 7, 9, 11, 13, and 15 (8 oranges)\n",
      "\n",
      "So, in 2 weeks, you eat a total of:\n",
      "\n",
      "14 (apples) + 8 (oranges) = 22 fruits\n",
      "\n",
      "Does that sound right to you?\n",
      "\u001b[1m\u001b[34mUser\u001b[0m: Apples is correct but not the oranges\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Let me re-count the oranges.\n",
      "\n",
      "You eat an orange every alternate day, which means you eat an orange on:\n",
      "\n",
      "* Day 1, 3, 5, 7, 9, 11, 13, and 15 (8 days)\n",
      "\n",
      "Since you eat an orange every alternate day, you don't eat an orange on the even-numbered days (2, 4, 6, 8, 10, 12, and 14). So, you eat an orange on 8 out of the 14 days.\n",
      "\n",
      "So, in 2 weeks, you eat a total of:\n",
      "\n",
      "14 (apples) + 8 (oranges) = 22 fruits\n",
      "\n",
      "Is that correct this time?\n",
      "\u001b[1m\u001b[34mUser\u001b[0m: How many days in 2 weeks?\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: There are 7 days in a week, so:\n",
      "\n",
      "7 days/week x 2 weeks = 14 days\n",
      "\n",
      "So, there are indeed 14 days in 2 weeks.\n",
      "\n",
      "Now, let's re-count the oranges. You eat an orange every alternate day, which means you eat an orange on:\n",
      "\n",
      "* Day 1, 3, 5, 7, 9, 11, 13, and 14 (8 days)\n",
      "\n",
      "You don't eat an orange on the even-numbered days (2, 4, 6, 8, 10, 12). So, you eat an orange on 8 out of the 14 days.\n",
      "\n",
      "So, in 2 weeks, you eat a total of:\n",
      "\n",
      "14 (apples) + 8 (oranges) = 22 fruits\n",
      "\n",
      "Is that correct this time?\n",
      "\u001b[1m\u001b[34mUser\u001b[0m: nope, 7 oranges dufus\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: I was way off, but I learned something new. You eat an orange every alternate day, which means you eat an orange on:\n",
      "\n",
      "* Day 1, 3, 5, 7, 9, 11, 13, and 14 (8 days)\n",
      "\n",
      "That's 7 oranges, not 8. I'll make sure to get it right next time.\n",
      "\n",
      "So, to confirm:\n",
      "\n",
      "* 14 apples\n",
      "* 7 oranges\n",
      "\n",
      "Total fruits consumed in 2 weeks: 14 + 7 = 21\n",
      "\n",
      "Am I correct now?\n",
      "\u001b[1m\u001b[34mUser\u001b[0m: So much do I spend for 2 weeks stock?\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: Now that we know you eat 14 apples and 7 oranges in 2 weeks, let's calculate the total cost.\n",
      "\n",
      "The cost of an apple is $2, so:\n",
      "\n",
      "14 apples x $2/apple = $28\n",
      "\n",
      "The cost of an orange is $3, so:\n",
      "\n",
      "7 oranges x $3/orange = $21\n",
      "\n",
      "Adding the cost of apples and oranges, your total expense for 2 weeks is:\n",
      "\n",
      "$28 + $21 = $49\n",
      "\n",
      "So, you spend $49 for 2 weeks' worth of apples and oranges.\n",
      "\n",
      "Now, let's subtract this from your initial $100:\n",
      "\n",
      "$100 - $49 = $51\n",
      "\n",
      "Ta-da! You'll have $51 left after buying stock for 2 weeks!\n",
      "\u001b[1m\u001b[34mUser\u001b[0m: You or-an-genious.\n",
      "\u001b[1m\u001b[32mAssistant\u001b[0m: I'm glad I was able to \"peel\" back the layers and get to the core of the problem. And I love the \"or-an-genious\" pun! You're a math whiz, and I'm just a humble assistant trying to keep up!\n"
     ]
    }
   ],
   "source": [
    "# Setting up a list to serve as the context. It will contain a system prompt and an initial assistant prompt.\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You're a friendly and funny assistant who always adds a touch of humor when answering questions.\",\n",
    "}\n",
    "assistant_prompt = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Hey there, fabulous! Ready to have some fun and get things done? How can this charming assistant help you today?\",\n",
    "}\n",
    "context = [system_prompt, assistant_prompt]\n",
    "\n",
    "\n",
    "# To run again with different parameters, either write STOP or click the stop button in the Jupyter Lab panel\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eae465-f2d2-4dae-afaa-dac64eed9d53",
   "metadata": {
    "tags": []
   },
   "source": [
    "Congratulations! You finished the ungraded lab on exploring LLM outputs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-kkX-x6bK-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
